# Analyse von Tieren mittels Keypoint Detektoren

Dies ist das Mono Repo zum AusfÃ¼hren der Ausarbeitungen von der Bachelorarbeit `Implementierung und Evaluation von AnsÃ¤tzen zur sensorbasierten Analyse von Tieren mittels Key-Point Detektion`
## Verzeichnisstruktur ğŸ“‚

```
monorepo/
â”‚
â”œâ”€â”€ annotation_tool/
â”‚   â”œâ”€â”€ annotation_tool.py
â”‚   â”œâ”€â”€ augment-script.py
â”‚   â”œâ”€â”€ create_data.py
â”‚
â”œâ”€â”€ cv/
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ processing.py
â”‚   â”œâ”€â”€ plotting.py
â”‚   â”œâ”€â”€ video_utils.py
â”‚   â”œâ”€â”€ initialization.py
â”‚   â”œâ”€â”€ inference.py
â”‚   â”œâ”€â”€ overlay.py
â”‚
â”œâ”€â”€ hoovenet/
â”‚   â”œâ”€â”€ model.py
â”‚   â”œâ”€â”€ train.py
â”‚   â”œâ”€â”€ utils.py
â”‚   â”œâ”€â”€ best_models/
â”‚
â”œâ”€â”€ common/
â”‚   â”œâ”€â”€ utils.py
â”‚   â”œâ”€â”€ constants.py
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ datasets/
â”‚   â”‚   â”œâ”€â”€ labeled/
â”‚   â”‚   â”œâ”€â”€ raw/
â”‚   â”œâ”€â”€ annotations.csv
â”‚   â”œâ”€â”€ labeled_frames.txt
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## Anforderungen ğŸ› ï¸

Bevor du loslegst, stelle sicher, dass du alle benÃ¶tigten Pakete installiert hast. Die Hauptanforderungen findest du in der `requirements.txt` Datei im Stammverzeichnis dieses Repos. Installiere sie einfach mit:

```
pip install -r requirements.txt
```

ZusÃ¤tzlich brauchst du mmpose. Benutze die Installations-Anleitung von [hier](https://mmpose.readthedocs.io/en/latest/installation.html) 

## Projekte ğŸš€

#### Wichtig: 
In die Verzeichnisse `data/dataset/raw` mÃ¼ssen noch die rohen Daten, des Datensatzes. FÃ¼r den Datensatz Horse-10 wurde ein Script `create_data.py` erstellt, dass die ganzen Subfolder auflÃ¶st und es zu einer Ein-Ordnerstruktur macht.


### Annotation Tool âœï¸

Das Annotation Tool bietet eine grafische BenutzeroberflÃ¤che, mit der du Videoframes ganz einfach mit HufzustÃ¤nden annotieren kannst.

- **Verzeichnis**: `annotation_tool/`
- **Starten**: Navigiere zum Verzeichnis `annotation_tool` und fÃ¼hre das Skript `annotation_tool.py` aus:

```
python annotation_tool.py
```

Die produzierten Daten kÃ¶nnen dann mithilfe des `augment-script.py` augmentiert werden so das mehr Daten und eine bessere generalisierbarkeit/ robustheit vorhanden ist.
Hier mÃ¼ssen aber unbedingt die Konstanten angepasst werden um auf die richtigen Verzeichnisse zu zeigen!

### Computer Vision Pipeline ğŸ“¹

Die Computer Vision Pipeline verarbeitet Videos, extrahiert Keypoints und erstellt verschiedene Visualisierungen und Overlays.

- **Verzeichnis**: `cv/`
- **Starten**: Navigiere zum Verzeichnis `cv` und fÃ¼hre das Skript `main.py` aus:

```
python main.py
```

### Hoovenet ğŸ§ 

Hoovenet umfasst das Training und die Verwendung eines neuronalen Netzes zur Vorhersage von HufzustÃ¤nden.

- **Verzeichnis**: `hoovenet/`
- **Training starten**: Navigiere zum Verzeichnis `hoovenet` und fÃ¼hre das Skript `train.py` aus:

```
python train.py
```

### Gemeinsame Bibliothek ğŸ“š

Gemeinsame Funktionen und Konstanten, die von verschiedenen Projekten verwendet werden.

- **Verzeichnis**: `common/`

## Konfiguration und Anpassung âš™ï¸

Alle konfigurierbaren Konstanten sind in `common/constants.py` definiert. Du kannst dort Pfade, Videoeinstellungen und andere Parameter anpassen.

## Datenverzeichnis ğŸ“

- **`data/`**: Hier findest du die DatensÃ¤tze, Frames, annotierten Frames und zugehÃ¶rige Dateien. Es ist wichtig in `data/dataset/raw` die un-gelabelten Bilder hereinzulegen. Das Model wurde auf dem Datensatz Horse-10 ([Huggingface](https://huggingface.co/datasets/mwmathis/Horse-30)) vortrainiert. Falls andere Bilder zum Trainieren verwendet werden, verwende nicht das Vortrainierte Model.


Diese AbhÃ¤ngigkeiten, bis auf MMpose sind in der `requirements.txt` Datei im Stammverzeichnis des Monorepos aufgefÃ¼hrt.